import os, sys, copy, sklearn, shutil, random, itertools
from sklearn.linear_model import RidgeCV
from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier
from mpl_toolkits.mplot3d import Axes3D
import matplotlib
from matplotlib.ticker import LinearLocator, FormatStrFormatter
from pyfitit import *
from scipy.optimize import minimize
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets
import scipy.signal
import scipy.interpolate
from scipy.spatial import distance
from sklearn.utils.testing import ignore_warnings
from sklearn.exceptions import ConvergenceWarning
if utils.isLibExists("lightgbm"):
    import lightgbm as lgb


def plotting_spectra(sample, to_delete, centering, normalize):
    energy, spectra, parameters = sample.energy, sample.spectra.values.T, sample.params.values
    if centering:
        spectra = spectra - np.mean(spectra, axis=1, keepdims=True)
        if normalize:
            spectra = spectra / np.std(spectra, axis=1, keepdims=True)
    if to_delete is not None:
        spectra = np.delete(spectra,np.sort(to_delete),1)
        parameters = np.delete(parameters,np.sort(to_delete),0)

    def Range(s):
        v1 = s[0]
        v2 = s[1]
        fig,ax = pplotting.createfig(interactive=True, figsize=(13,5))
        ax.plot(energy[v1:v2], spectra[v1:v2], linewidth=0.1, color='blue')
        ax.set_xlabel('Energy', fontweight='bold')
        ax.set_ylabel('Absorption', fontweight='bold')
        ax.axvline(x=energy[v1], color='black', linestyle='--')
        ax.axvline(x=energy[v2], color='black', linestyle='--')
        ax.grid()

    _ = interact(Range,s=widgets.IntRangeSlider(
            value=[0,len(energy)-1],
            min=0,
            max=len(energy)-1,
            step=1,
            description='Energy:',
            disabled=False,
            continuous_update=False,
            orientation='horizontal',
            readout=True,
            readout_format='d',
        ))
    p = pd.DataFrame(data=parameters, columns=sample.params.columns)
    s = pd.DataFrame(data=spectra.T, columns=sample.spectra.columns)
    res = ML.Sample(p, s)
    return res


def error(spectra,nPCs):
    u,s,vT=np.linalg.svd(spectra,full_matrices=False)
    ur=u[:,:nPCs]
    sr=np.diag(s[:nPCs])
    vr=vT[:nPCs,:]
    #print('Ur:',ur.shape,'Sr:',sr.shape,'Vr:',vr.shape)
    snew=np.dot(np.dot(ur,sr),vr)
    return np.mean(np.sqrt(np.mean((spectra-snew)**2, axis=1))/np.sqrt(np.mean((spectra)**2, axis=1)))


def plot_error(s,error_plot):
    fig,ax = plotting.createfig(interactive=True, nrows=1, ncols=2, figsize=(13,5))
    ax[0].plot(np.arange(1,21),s[0:20],'-o',label='Scree Plot',color='blue')
    ax[0].set_xlabel('PCs',fontweight='bold')
    ax[0].set_ylabel('Singular Values',fontweight='bold')
    ax[0].grid()
    ax[1].plot(np.arange(1,21),error_plot,'-o',label='Scree Plot',color='red')
    ax[1].set_xlabel('PCs',fontweight='bold')
    ax[1].set_ylabel('Error',fontweight='bold')
    ax[1].grid()

def statistic(spectra,log_scale,number=20):
    u,s,v=np.linalg.svd(spectra)
    v=v.T
    error_plot=[]
    for i in range(0,number):
        error_plot.append(error(spectra,i))

    if log_scale==True:
        s=np.log10(s)
        error_plot=np.log10(error_plot)
        plot_error(s,error_plot)
    elif log_scale==False:
        s=s
        error_plot=error_plot
        plot_error(s,error_plot)
    elif log_scale !=False and log_scale !=True:
        print('Error in log_scale name')

def cross_val_predict(method, X, y):
    cv = sklearn.model_selection.KFold(n_splits=7, shuffle=True, random_state=0)
    res = np.zeros(y.shape)
    for train_index, test_index in cv.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        method.fit(X_train, y_train)
        res[test_index] = method.predict(X_test).reshape(-1)
    return res


def buildDescriptor(spectra, descriptor_type='PCA'):
    if descriptor_type == 'PCA':
        u,s,vT=np.linalg.svd(spectra,full_matrices=False)
        s=np.diag(s)
        C = np.dot(s,vT).T
    elif descriptor_type == 'intervals average':
        m = 10
        spectraT = spectra.T
        C = np.zeros((spectraT.shape[0], m))
        for i in range(m):
            ne = spectraT.shape[1]
            C[:,i] = np.mean(spectraT[:, ne*i//m:ne*(i+1)//m], axis=1)
    else: raise Error('Error in typing regressor name')
    return C


def training(descriptor, parameters, regressor_name, number=10):
    if number>descriptor.shape[1]: number = descriptor.shape[1]
    clf = [None]*number
    for i in range(number):
        y = descriptor[:,i]
        if regressor_name=='Ridge':
            clf[i] = RidgeCV()
        elif regressor_name=='ExtraTrees':
            clf[i]=ExtraTreesRegressor(n_estimators=100,random_state=0,min_samples_leaf=10)
        elif regressor_name=='RBF':
            clf[i] = ML.RBF(function='linear', baseRegression='linear')
        else:
            print('Error in typing regressor name')
        clf[i].fit(parameters, y)
    return clf

def plot_training_error(descriptor, parameters, clf):
    graf=[]
    number = len(clf)
    for i in range(number):
        y = descriptor[:,i]
        pred = cross_val_predict(clf[i], parameters, y)
        graf.append(sklearn.metrics.r2_score(y, pred)*100)
    fig,ax = plotting.createfig(interactive=True, figsize=(13,5))
    ax.plot(np.arange(1,number+1),graf,'-o')
    ax.set_ylabel('Quality %',fontweight='bold')
    ax.set_xlabel('PCs',fontweight='bold')
    ax.grid()

def create_C_plot(val1,val2,dim_param,npt,PC,min_val,max_val):
    if val1 == val2:
        print('Error: The two values are the same')
    elif np.remainder(val1,1) !=0 or np.remainder(val2,1) !=0:
        print('Chosed not integer values')
    else:
        matrix_values=np.zeros((npt**2,dim_param))
        var_vector=np.array(np.meshgrid(np.linspace(min_val, max_val,npt), np.linspace(min_val, max_val,npt))).T.reshape(-1,2)
        matrix_values[:,val1]=var_vector[:,0]
        matrix_values[:,val2]=var_vector[:,1]
        matrix_grid=np.zeros(matrix_values.shape[0])
        for i in range(matrix_values.shape[0]):
            matrix_grid[i]=clf[PC].predict(matrix_values[i].reshape(1,-1))
        return matrix_grid.reshape(npt,npt)

def twoDmap(clf,param,npt,min_val,max_val):
    dim_param=np.shape(param.values)[1]
    def selectPC(number):
        control=number
        def selectP1(s1):
            val1=s1
            def selectP2(s2):
                val2=s2
                if val1 == val2:
                    return(print('The two values are the same'))
                else:
                    matrix_values=np.zeros((npt**2,dim_param))
                    var_vector=np.array(np.meshgrid(np.linspace(min_val, max_val,npt), np.linspace(min_val, max_val,npt))).T.reshape(-1,2)
                    matrix_values[:,val1]=var_vector[:,0]
                    matrix_values[:,val2]=var_vector[:,1]
                    matrix_grid=np.zeros(matrix_values.shape[0])
                    for i in range(matrix_values.shape[0]):
                        matrix_grid[i]=clf[number].predict(matrix_values[i].reshape(1,-1))
                    x= np.linspace(min_val, max_val,npt)
                    y= np.linspace(min_val, max_val,npt)
                    X,Y=np.meshgrid(x,y)
                    Z=matrix_grid.reshape(npt,npt)
                    dgy,dgx=np.gradient(Z)
                    fig = plt.figure(figsize=(13,5))
                    ax = fig.gca()
                    surf=ax.contourf(X, Y, Z, 30,cmap='Spectral')
                    ax.tick_params(direction='in',labelsize=15,width=2)
                    ax.set_xlabel(param.keys()[val1],fontweight='bold')
                    ax.set_ylabel(param.keys()[val2],fontweight='bold')
                    fig.colorbar(surf)
                    ax.streamplot(X, Y, dgx,dgy,color='black')
            _=interact(selectP2, s2 = widgets.BoundedIntText(value=1, min=0, max=dim_param-1, step=1, description='p2:', disabled=False))
        _=interact(selectP1, s1 = widgets.BoundedIntText(value=0, min=0, max=dim_param-1, step=1, description='p1:', disabled=False))
    _=interact(selectPC, number = widgets.BoundedIntText(value=0, min=0, max=20, step=1, description='PC:', disabled=False))

def iso(parameters,PC,C,clf):
    return (C-clf[PC].predict(parameters.reshape(1,-1))[0])**2

def find_isosurface(param,min_val,max_val,clf,iso_param):
    PC=iso_param[0]
    C=iso_param[1]
    p1=iso_param[2]
    p2=iso_param[3]
    #C = C_vector[PC]
    #C = C_values
    dim=np.shape(param.values)[1]
    p_values=np.zeros((1000,2))
    bnds=np.zeros((dim,2))
    for i in range(len(bnds)):
        bnds[p1]=[min_val,max_val]
        bnds[p2]=[min_val,max_val]
    for i in range(1000):
        p=np.zeros(dim)
        p[p1]=random.uniform(-0.2, 0.2)
        p[p2]=random.uniform(-0.2, 0.2)
        #res=minimize(iso, p,args=(PC,C),bounds=bnds)#bounds=bnds)
        res=minimize(iso, p,args=(PC,C,clf),bounds=bnds)
        p_values[i][0]=res.x[p1]
        p_values[i][1]=res.x[p2]
    return p_values

def plot_isosurfaces(g1,g2,iso_param_1,iso_param_2,param):
    fig,ax=plotting.createfig(interactive=True, nrows=1, ncols=3, figsize=(13,5))
    if iso_param_1[2] != iso_param_2[2] or iso_param_1[3] != iso_param_2[3]: print('Error Not common parameters chosen for the plot')
    else:
        ax[0].set_title('PC:'+str(iso_param_1[0]))
        ax[0].plot(g1[:,0],g1[:,1],'o')
        ax[0].set_xlim(-0.22,0.22)
        ax[0].set_ylim(-0.22,0.22)
        ax[0].set_xlabel(param.keys()[iso_param_1[2]],fontweight='bold')
        ax[0].set_ylabel(param.keys()[iso_param_1[3]],fontweight='bold')
        ax[0].grid()
        ax[1].set_title('PC:'+str(iso_param_2[0]))
        ax[1].plot(g2[:,0],g2[:,1],'o',color='orange')
        ax[1].set_xlim(-0.22,0.22)
        ax[1].set_ylim(-0.22,0.22)
        ax[1].set_xlabel(param.keys()[iso_param_1[2]],fontweight='bold')
        ax[1].set_ylabel(param.keys()[iso_param_1[3]],fontweight='bold')
        ax[1].grid()
        ax[2].set_title('Merging Plot')
        ax[2].plot(g1[:,0],g1[:,1],'o')
        ax[2].plot(g2[:,0],g2[:,1],'o',color='orange')
        ax[2].set_xlabel(param.keys()[iso_param_1[2]],fontweight='bold')
        ax[2].set_ylabel(param.keys()[iso_param_1[3]],fontweight='bold')
        ax[2].grid()


def findExtrema(sample, extremaType, energyInterval, maxRf=.1, allowExternal=True, maxExtremumPointsDist=5, intensityNormForMaxExtremumPointsDist=1, maxAdditionIter=-1, refineExtremum=True, extremumInterpolationRadius=10, returnIndices=False, plotToFile=None):
    """
        finds descriptors for XANES extrema

    :param extremumInterpolationRadius:
        Radius of interpolation, only used if refineExtremum=True
    :param refineExtremum:
        Interpolate extremum's neighbourhood in order to find more accurate position of extremum
    :param maxAdditionIter:
        Amount af bad to good transfer iterations
    :param maxExtremumPointsDist:
        Max euclidean distance between good and bad extrema points (y is normed to intensityNormForMaxExtremumPointsDist)
    :param allowExternal:
        Allow searching of bad extrema outside of energyInterval
    :param maxRf:
        Max R-Factor between good and bad spectra
    :param energyInterval: Tuple
        Energy range inside which to search extrema
    :param sample: ML.Sample or tuple (intensities, energy). Each row is a spectrum
    :param extremaType: str
        'min' for pits, 'max' for peaks
    :return:
        newSample (only good spectra), descriptors (extrema_x,y, diff2), and goodSpectrumIndices if returnIndices=True
    """

    assert extremaType in ['min', 'max']

    def calculateRFactor(energy, xanes1, xanes2):
        return utils.integral(energy, (xanes1 - xanes2) ** 2) / \
               utils.integral(energy, xanes1 ** 2)

    def divideSpectra(energy, extremaIndices, energyInterval):
        # allowedIndices = np.logical_and(energy >= energyInterval[0], energy <= energyInterval[1])
        good = []
        bad = []
        spectrumIndex = 0
        for extremumIndices in extremaIndices:
            extremumCount = 0
            validExtremumIndex = -1
            for extremumIndex in extremumIndices:
                if isIndexInRange(energy, extremumIndex, energyInterval):
                    extremumCount += 1
                    validExtremumIndex = extremumIndex
            if extremumCount == 1:
                good.append((spectrumIndex, validExtremumIndex))
            else:
                bad.append(spectrumIndex)
            spectrumIndex += 1

        return good, bad

    def closestExtremumIndex(extremumEnergy, extrema, energy):
        if len(extrema) == 0:
            return -1

        extremaEnergies = list(map(lambda x: abs(extremumEnergy - energy[x]), extrema))
        return np.argmin(extremaEnergies)

    def isIndexInRange(energy, energyIndex, energyInterval):
        return (energy[energyIndex] >= energyInterval[0]) and (energy[energyIndex] <= energyInterval[1])

    def moveBadSpectraToGood(good, bad, energy, intensities, extrema, energyInterval):
        if len(good) == 0 or len(bad) == 0:
            return good, bad

        goodSpectra = list(map(lambda x: intensities[x[0]], good))
        badSpectra = list(map(lambda x: intensities[x], bad))
        d = distance.cdist(badSpectra, goodSpectra, lambda x, y: calculateRFactor(energy, x, y))

        newGood = copy.deepcopy(good)
        newBad = copy.deepcopy(bad)
        sortedIndices = np.argsort(d)
        goodExtrIntensities = list(map(lambda x: intensities[x[0]][x[1]], good))
        for badIndex in range(d.shape[0]):
            for goodIndex in sortedIndices[badIndex]:
                # filtering those which exceed max rfactor
                if d[badIndex][goodIndex] > maxRf:
                    break

                globalBadIndex = bad[badIndex]
                globalGoodIndex = good[goodIndex][0]
                goodExtremumEnergy = energy[good[goodIndex][1]]

                closestIndex = closestExtremumIndex(goodExtremumEnergy, extrema[globalBadIndex], energy)
                if closestIndex<0: continue
                badExtrEnergyIndex = extrema[globalBadIndex][closestIndex]
                badExtrPoint = np.array((energy[badExtrEnergyIndex], intensities[globalBadIndex][badExtrEnergyIndex] / intensityNormForMaxExtremumPointsDist))
                goodExtrPoint = np.array((goodExtremumEnergy, intensities[globalGoodIndex][good[goodIndex][1]] / intensityNormForMaxExtremumPointsDist))
                dist = np.linalg.norm(badExtrPoint - goodExtrPoint)

                # filtering by distance
                if dist > maxExtremumPointsDist or \
                   not allowExternal and not isIndexInRange(energy, closestIndex, energyInterval):
                    continue

                # move filtered bad spectrum from 'bad' to 'good' list
                newGood.append((globalBadIndex, badExtrEnergyIndex))
                newBad.remove(globalBadIndex)
                break

        return newGood, newBad

    def getFinalExtremaAndDerivatives(good, energy, intensities, refineExtremum, extremumInterpolationRadius, comparator):
        if not refineExtremum:
            xExtrema = np.array(list(map(lambda pair: energy[pair[1]], good)))
            yExtrema = np.array(list(map(lambda pair: intensities[pair[0]][pair[1]], good)))

            def f(x):
                (sIndex, extremumIndex) = x
                intensity = intensities[sIndex]
                d1Left = (intensity[extremumIndex] - intensity[extremumIndex - 1]) / (energy[extremumIndex] - energy[extremumIndex - 1])
                d1Right = (intensity[extremumIndex + 1] - intensity[extremumIndex]) / (energy[extremumIndex + 1] - energy[extremumIndex])
                leftMeanEnergy = (energy[extremumIndex] - energy[extremumIndex - 1]) / 2
                rightMeanEnergy = (energy[extremumIndex + 1] - energy[extremumIndex]) / 2
                return (d1Right - d1Left) / (rightMeanEnergy - leftMeanEnergy)

            derivatives = np.apply_along_axis(f, 1, np.array(good))
            return np.array([xExtrema, yExtrema]), derivatives

        xStep = 0.01
        xExtrema = []
        yExtrema = []

        derivative = []
        for (spectrumIndex, extremumIndex) in good:
            xnew = np.arange(energy[extremumIndex] - extremumInterpolationRadius, energy[extremumIndex] + extremumInterpolationRadius, xStep)
            spline = scipy.interpolate.CubicSpline(energy, intensities[spectrumIndex])
            ynew = spline(xnew)
            extrema = np.array(scipy.signal.argrelextrema(ynew, comparator)[0])
            index = np.argsort(abs(xnew[extrema] - energy[extremumIndex]))[0]
            newExtremumIndex = extrema[index]
            xExtrema.append(xnew[newExtremumIndex])
            yExtrema.append(ynew[newExtremumIndex])
            # assert abs(yExtrema[-1] - np.interp(xExtrema[-1], energy, intensities[spectrumIndex])) < 0.1

            derivative.append((ynew[newExtremumIndex - 1] - 2*ynew[newExtremumIndex] + ynew[newExtremumIndex+1]) / xStep ** 2)

        return np.array([np.array(xExtrema), np.array(yExtrema)]), np.array(derivative)

    def plot(good, bad, energy, intensities, extremaPoints):
        if plotToFile is None:
            return

        goodBadIndices = []
        for (i, e) in good:
            goodBadIndices.append((i, e, 'good'))
        for i in bad:
            goodBadIndices.append((i, -1, 'bad'))
        # random.shuffle(goodBadIndices)

        fig, ax = plotting.createfig()
        alpha = min(1/len(goodBadIndices)*100,1)
        for (spectrum, extremum, label) in goodBadIndices:
            if label == 'good':
                ax.plot(energy, intensities[spectrum], color='blue', lw=.3, alpha=alpha)
        for x, y in zip(extremaPoints[0], extremaPoints[1]):
            ax.plot(x, y, marker='o', markersize=3, color="red", alpha=alpha)
        for (spectrum, extremum, label) in goodBadIndices:
            if label == 'bad':
                ax.plot(energy, intensities[spectrum], color='black', lw=.3)
        plotting.savefig(plotToFile, fig)
        plotting.closefig(fig)

    def ensureCorrectResults(intensities, good, bad, extremaPoints, derivatives):
        goodIndices = list(map(lambda x: x[0], good))
        assert len(goodIndices) == len(set(goodIndices)), 'indistinct good values'
        assert len(bad) == len(set(bad)), 'indistinct bad values'
        assert len(bad) + len(good) == len(intensities), 'spectra inconsistency'
        assert extremaPoints[0].shape[0] == extremaPoints[1].shape[0] and \
               extremaPoints[0].shape[0] == derivatives.shape[0], 'bad descriptor integrity'

    def findExtremaIndices(intensities, comparator):
        extrema = map(lambda x: scipy.signal.argrelextrema(x, comparator)[0], intensities)
        return list(extrema)

    # main function
    if isinstance(sample, ML.Sample):
        energy = sample.energy
        intensities = sample.spectra.values
    else: #tuple 
        (intensities, energy) = sample
    comparator = np.less if extremaType == 'min' else np.greater
    # getting indices of energies in which extrema are present
    extrema = findExtremaIndices(intensities, comparator)

    good, bad = divideSpectra(energy, extrema, energyInterval)
    print('First split: ', len(good), len(bad))

    # try to identify good spectra in bad list and move them
    iterations = 0
    while iterations < maxAdditionIter or maxAdditionIter == -1:
        newGood, newBad = moveBadSpectraToGood(good, bad, energy, intensities, extrema, energyInterval)
        added = len(newGood) - len(good)
        print('Iteration = %i, added %i spectra' % (iterations + 1, added))
        shouldContinue = added > 0
        good = newGood
        bad = newBad
        iterations += 1

        if not shouldContinue:
            break
    good = sorted(good, key=lambda pair: pair[0])
    extremaPoints, derivatives = getFinalExtremaAndDerivatives(good, energy, intensities, refineExtremum, extremumInterpolationRadius, comparator)
    goodSpectrumIndices = list(map(lambda x: x[0], good))
    descr = np.vstack((extremaPoints, derivatives)).T

    # plot results
    plot(good, bad, energy, intensities, extremaPoints)

    ensureCorrectResults(intensities, good, bad, extremaPoints, derivatives)
    if isinstance(sample, ML.Sample):
        newSpectra = sample.spectra.loc[goodSpectrumIndices].reset_index(drop=True)
        newParams = sample.params.loc[goodSpectrumIndices].reset_index(drop=True)
        newSample = ML.Sample(newParams, newSpectra)
    else: newSample = intensities[goodSpectrumIndices]

    if returnIndices:
        return newSample, descr, goodSpectrumIndices
    else:
        return newSample, descr


def findExtremumByFit(spectrum, energyInterval, fitByPolynomDegree = 2, returnPolynom = False):
    """
    :param spectrum:
    :param energyInterval: 
    :param fitByPolynomDegree: 
    :param returnPolynom: 
    :param extremaType: 'min' or 'max'
    :returns: descriptors [(extremum energy, extremum value, 2d derivative at extremum)] sorted by 2d derivative
    """
    x = spectrum.energy
    ind = (x >= energyInterval[0]) & (x <= energyInterval[1])
    x = x[ind]
    y = spectrum.intensity[ind]
    plt.plot(x, y)

    poly = np.polynomial.polynomial.Polynomial.fit(x, y, fitByPolynomDegree)
    dpoly = poly.deriv(1)
    extrema = dpoly.roots()
    extrema_val = poly(extrema)
    d2poly = poly.deriv(2)
    d2val = d2poly(extrema)
    ind = np.argsort(d2val)
    descriptors = (extrema[ind], extrema_val[ind], d2val[ind])

    if returnPolynom:
        return descriptors, poly
    else:
        return descriptors


def stableExtrema(spectra, energy, extremaType, energyInterval, plotResultToFolder=None, maxRf=.1, allowExternal=True, maxExtremumPointsDist=5, intensityNormForMaxExtremumPointsDist=1, maxAdditionIter=-1, refineExtremum=True, extremumInterpolationRadius=10, smoothRad=5):
    assert extremaType in ['min', 'max'], 'invalid extremaType'
    spectra1 = np.copy(spectra)
    for i in range(len(spectra)):
        spectra1[i] = smoothLib.simpleSmooth(energy, spectra1[i], smoothRad, kernel='Gauss')
    newSpectra, descr, goodSpectrumIndices = findExtrema((spectra1, energy), extremaType, energyInterval, maxRf=maxRf, allowExternal=allowExternal, maxExtremumPointsDist=maxExtremumPointsDist, intensityNormForMaxExtremumPointsDist=intensityNormForMaxExtremumPointsDist, maxAdditionIter=maxAdditionIter, refineExtremum=refineExtremum, extremumInterpolationRadius=extremumInterpolationRadius, returnIndices=True, plotToFile='stableExtrema-'+extremaType+'.png')
    if len(newSpectra) != len(spectra):
        warnings.warn(f'Can\'t find {extremaType} for {len(spectra)-len(newSpectra)} spectra. Try changing search interval or expand energy interval for all spectra. See plot for details.')
    if plotResultToFolder is not None:
        extrema_x = descr[:,0]
        extrema_y = descr[:,1]
        for i1 in range(min(100, len(spectra))):
            if i1 == 0:
                if os.path.exists(plotResultToFolder): shutil.rmtree(plotResultToFolder)
                os.makedirs(plotResultToFolder, exist_ok=True)
            # i = np.random.randint(0,len(spectra))
            i = i1
            fig, ax = plotting.createfig()
            ax.plot(energy, spectra1[i], label='stable smooth')
            ax.plot(energy, spectra[i], label='spectrum')
            d = np.max(spectra[i])-np.min(spectra[i])
            ax.set_ylim(np.min(spectra[i])-d*0.1, np.max(spectra[i])+d*0.1)
            ax.scatter([extrema_x[i]], [extrema_y[i]], 10)
            ax.legend()
            plotting.savefig(plotResultToFolder+os.sep+str(i)+'.png', fig)
            plotting.closefig(fig)
    return descr, goodSpectrumIndices


def stableExtremaOld(spectra, energy, extremaType, energyInterval, fitByPolynomDegree=2, plotResultToFolder=None, plotBadOnly=True):
    assert extremaType in ['min', 'max'], 'invalid extremaType'
    assert fitByPolynomDegree in [2,3]
    extrema_x = np.zeros(len(spectra))
    extrema_y = np.zeros(len(spectra))
    extrema_sharpness = np.zeros(len(spectra))
    for i in range(len(spectra)):
        spectrum = utils.Spectrum(energy, spectra[i])
        if plotResultToFolder is None:
            extrema, extrema_val, d2val = findExtremumByFit(spectrum, energyInterval, fitByPolynomDegree)
        else:
            ds, poly = findExtremumByFit(spectrum, energyInterval, fitByPolynomDegree, returnPolynomGraph=True)
            extrema, extrema_val, d2val = ds
        if fitByPolynomDegree==2:
            if extremaType == 'min':
                assert d2val[0]>=0, 'There is no min for spectrum '+str(i)+' on interval '+str(energyInterval)
            else:
                assert d2val[0]<=0, 'There is no max for spectrum '+str(i)+' on interval '+str(energyInterval)
        j = -1 if extremaType == 'min' else 0
        extrema_x[i] = extrema[j]
        extrema_y[i] = extrema_val[j]
        extrema_sharpness[i] = d2val[j]
        if plotResultToFolder is not None:
            if i == 0:
                if os.path.exists(plotResultToFolder): shutil.rmtree(plotResultToFolder)
                os.makedirs(plotResultToFolder, exist_ok=True)
            if plotBadOnly and (extrema_x[i]<energy[0] or extrema_x[i]>energy[-1]):
                fig, ax = plotting.createfig()
                ax.plot(energy, poly(energy), label='polynom')
                ax.plot(energy, spectra[i], label='spectrum')
                d = np.max(spectra[i])-np.min(spectra[i])
                ax.set_ylim(np.min(spectra[i])-d*0.1, np.max(spectra[i])+d*0.1)
                ax.scatter([extrema_x[i]], [extrema_y[i]], 10)
                ax.legend()
                plotting.savefig(plotResultToFolder+os.sep+str(i)+'.png', fig)
                plotting.closefig(fig)
    return np.array([extrema_x,extrema_y,extrema_sharpness]).T


def pcaDescriptor(spectra, count=None):
    """Build pca descriptor.
    
    :param spectra: 2-d matrix of spectra (each row is one spectrum)
    :param count: count of pca descriptors to build (default: all)
    :returns: 2-d matrix (each column is one descriptor values for all spectra)
    """
    u,s,vT = np.linalg.svd(spectra.T,full_matrices=False)
    s = np.diag(s)
    C = np.dot(s,vT).T
    if count is not None: C = C[:,:min([C.shape[1],count])]
    return C


def relPcaDescriptor(spectra, energy0, Efermi, count=None):
    """Build relative pca descriptor. It is a pca descriptor for aligned spectra
    
    :param spectra: 2-d matrix of spectra (each row is one spectrum)
    :param energy: energy values (common for all spectra)
    :param Efermi: Efermi values for all spectra
    :param count: count of pca descriptors to build (default: all)
    :returns: 2-d matrix (each column is one descriptor values for all spectra)
    """
    spectra_rel = copy.deepcopy(spectra)
    energy = energy0 - Efermi[0]
    for i in range(len(spectra_rel)):
        spectra_rel[i] = np.interp(energy, energy0-Efermi[i], spectra_rel[i])
    return pcaDescriptor(spectra_rel, count)


def efermiDescriptor(spectra, energy):
    """Find Efermi and arctan grow rate for all spectra and returns as 2 column matrix
    
    :param spectra: 2-d matrix of spectra (each row is one spectrum)
    """
    d = np.zeros((spectra.shape[0],2))
    for i in range(spectra.shape[0]):
        arcTanParams, _ = curveFitting.findEfermiByArcTan(energy, spectra[i])
        d[i] = [arcTanParams['x0'], arcTanParams['a']]
    return d


def addDescriptors(sample, descriptors):
    """
    :param sample: Sample instance
    :param descriptors: list of str (descriptor type) or dict{'type':.., 'columnName':.., 'arg1':.., 'arg2':.., ...}. Possible descriptor types: 'stableExtrema' (params - see. stableExtrema function), 'efermi', 'pca', 'rel_pca'
    :return: new sample with descriptors
    """
    # canonizations
    newD = []
    for d in descriptors:
        if isinstance(d, str): newD.append({'type':d})
        else: newD.append(d)
    descriptors = newD
    spectra = sample.spectra.to_numpy()
    energy = sample.energy
    data = sample.params
    for d in descriptors:
        typ = d['type']
        params = copy.deepcopy(d)
        del params['type']
        if typ == 'stableExtrema':
            assert 'extremaType' in params
            name = params['columnName'] if 'columnName' in params else params['extremaType']
            ext_e = name+'_e'
            ext_int = name+'_int'
            ext_d2 = name+'_d2'
            assert (ext_e not in data.columns) and (ext_int not in data.columns) and (ext_d2 not in data.columns), f'Duplicate descriptor names while adding {ext_e}, {ext_int}, {ext_d2} to {data.columns}. Use columnName argument in descriptor parameters'
            ext, goodSpectrumIndices = stableExtrema(spectra, energy, **params)
            assert np.all(np.diff(goodSpectrumIndices) >= 0)
            if len(goodSpectrumIndices) != data.shape[0]:
                unknown = np.any(np.isnan(data.to_numpy()), axis=1).reshape(-1)
                unknown = np.where(unknown)[0]
                common = np.intersect1d(unknown, goodSpectrumIndices)
                assert len(common) == len(unknown), f"Can\'t find {params['extremaType']} for unknown spectra. Try changing search energy interval or expand energy interval for all spectra. See plot for details."
                data = data.loc[goodSpectrumIndices].reset_index(drop=True)
                spectra = spectra[goodSpectrumIndices,:]
            data[ext_e] = ext[:, 0]
            data[ext_int] = ext[:, 1]
            data[ext_d2] = ext[:, 2]
        elif typ == 'efermi':
            efermi = efermiDescriptor(spectra, energy)
            data['efermi'] = efermi[:, 0]
            data['efermiRate'] = efermi[:, 1]
        elif typ == 'pca':
            count = params['count'] if 'count' in params else 3
            pca = pcaDescriptor(spectra, count)
            data['pca1'] = pca[:, 0]
            data['pca2'] = pca[:, 1]
            data['pca3'] = pca[:, 2]
        elif typ == 'rel_pca':
            count = params['count'] if 'count' in params else 3
            if 'efermi' not in data.columns:
                efermi = efermiDescriptor(spectra, energy)
                efermi = efermi[:, 0]
            else: efermi = data['efermi']
            relpca = relPcaDescriptor(spectra, energy, efermi, count)
            data['rel_pca1'] = relpca[:, 0]
            data['rel_pca2'] = relpca[:, 1]
            data['rel_pca3'] = relpca[:, 2]
    return ML.Sample(data, spectra, energy)


def plot_descriptors_1d(data, spectra, energy, label_names, desc_points_names, folder):
    """Plot 1d graphs of descriptors vs labels
    
    Args:
        data (pandas dataframe):  data with desctriptors and labels
        spectra (TYPE): numpy 2d array of spectra (one row - one spectrum)
        energy (TYPE): energy values for spectra
        label_names (list): label names
        desc_points_names (list of pairs): points to plot on spectra graphs (for example: [[max energies, max intensities], [pit energies, pit intensities]])
        folder (TYPE): Description
    """
    if os.path.exists(folder): shutil.rmtree(folder)
    data = data.sample(frac=1).reset_index(drop=True)
    os.makedirs(folder, exist_ok=True)
    os.makedirs(folder+os.sep+'by_descriptor', exist_ok=True)
    os.makedirs(folder+os.sep+'by_label', exist_ok=True)
    descriptors = set(data.columns) - set(label_names)
    for label in label_names:
        os.makedirs(folder+os.sep+'by_label'+os.sep+label, exist_ok=True)
        if label not in data.columns: continue

        fig = plotting.plotSample(energy, spectra, color_param=data[label].values, sortByColors=False, fileName=None)
        ax = fig.axes[0]
        for desc_points in desc_points_names:
            ax.scatter(data[desc_points[0]], data[desc_points[1]], 10)
        ax.set_title('Spectra colored by '+label)
        plotting.savefig(folder+os.sep+'by_label'+os.sep+'xanes_spectra_'+label+'.png', fig)
        plotting.closefig(fig)

        for d in descriptors:
            os.makedirs(folder+os.sep+'by_descriptor'+os.sep+d, exist_ok=True)
            fig, ax = plotting.createfig()
            # known
            ind = pd.notnull(data[label])
            ma = np.max(data.loc[ind,label]); mi = np.min(data.loc[ind,label])
            delta = (np.random.rand(data.shape[0])*2-1)*(ma-mi)/30
            ax.scatter(data.loc[ind,d], data.loc[ind,label]+delta[ind], 600, c='black')
            ax.scatter(data.loc[ind,d], data.loc[ind,label]+delta[ind], 500, c='white')
            # text
            for i in range(data.shape[0]):
                if not np.isnan(data.loc[i,label]):
                    ax.text(data.loc[i,d], data.loc[i,label]+delta[i], str(i), ha='center', va='center', size=8)
            ax.set_xlabel(d)
            ax.set_ylabel(label)
            ax.set_xlim(plotting.getPlotLim(data.loc[ind,d]))
            ax.set_ylim(plotting.getPlotLim(data.loc[ind,label]+delta[ind]))
            plotting.savefig(folder+os.sep+'by_descriptor'+os.sep+d+os.sep+label+'.png', fig)
            plotting.savefig(folder+os.sep+'by_label'+os.sep+label+os.sep+d+'.png', fig)
            plotting.closefig(fig)


def isClassification(data, column):
    if data[column].dtype != 'float64': return True
    ind = pd.notnull(data[column])
    return np.all(np.round(data.loc[ind,column]) == data.loc[ind,column].to_numpy())


def getQuality(data, columns, label_names, model0=None, m=1, cv_count=10, returnModels=False, printDebug=True):
    """Get cross validation quality
    
    Args:
        data (pandas dataframe):  data with desctriptors and labels
        columns (list of strings): features to use (x)
        label_names (list of strings): labels (y)
        model0 (None, optional): model to use for prediction
        m (int, optional): number of cross validation attempts (each cv composed of cv_count parts)
        cv_count (int, optional): number of parts to divide dataset in cv
        returnModels (bool, optional): whether to return models
        printDebug (boolean): show debug output
    Returns:
        quality dict{label:quality}, predictions dict{label:array of predictions}, models dict{label:model}
    """
    quality = {}; quality_std = {}
    predictions = {}
    models = {}
    X = data.loc[:, columns].to_numpy()
    if printDebug:
        print('Try predict by columns:', columns)

    def getScore(model, X, y):
        try:
            with warnings.catch_warnings(record=True) as warn:
                pred = sklearn.model_selection.cross_val_predict(model, X, y, cv=sklearn.model_selection.KFold(cv_count, shuffle=True))
        except Warning:
            pass
        # print(classification, model, y, pred)
        res = sklearn.metrics.accuracy_score(y, pred) if classification else sklearn.metrics.r2_score(y, pred)
        return res, pred
    tryParams = [{'n_estimators':40, 'min_samples_leaf':4}]
    # if len(columns) == 1:
    tryParams.append({'n_estimators':40, 'min_samples_leaf':20})
    # model = ML.Normalize(sklearn.linear_model.LogisticRegression(multi_class='ovr'), xOnly=True)
    # model = ML.Normalize(sklearn.svm.SVC(C=1), xOnly=True)
    for name in label_names:
        assert name not in columns
        y = np.ravel(data[name].to_numpy())
        classification = isClassification(data, name)
        acc = np.zeros(m)
        # print(model0)
        if model0 is None:
            try_acc = np.zeros(len(tryParams))
            try_model = [None]*len(tryParams)
            for modelParams, j in zip(tryParams, range(len(tryParams))):
                if classification:
                    try_model[j] = sklearn.ensemble.ExtraTreesClassifier(**modelParams)
                else:
                    try_model[j] = sklearn.ensemble.ExtraTreesRegressor(**modelParams)
                # print(classification, try_model[j])
                try_acc[j], pred = getScore(try_model[j], X, y)
            bestj = np.argmax(try_acc)
            acc[0] = try_acc[bestj]
            model = try_model[bestj]
            if printDebug and len(tryParams)>1:
                print('Best model params: ', tryParams[bestj], f'Delta ={try_acc[bestj]-np.min(try_acc)}')
            for i in range(1,m):
                acc[i], pred = getScore(model, X, y)
        else:
            for i in range(m):
                model = copy.deepcopy(model0)
                acc[i], pred = getScore(model, X, y)
        quality[name] = np.mean(acc)
        quality_std[name] = np.std(acc)
        predictions[name] = pred
        if returnModels:
            try:
                with warnings.catch_warnings(record=True) as warn:
                    models[name] = model.fit(X,y)
            except Warning:
                pass
        cl = 'classification' if classification else 'regression'
        if printDebug:
            print('{} - {} score: {:.2f}-{:.2f}'.format(name, cl, np.min(acc), np.max(acc)))
    if printDebug: print('')
    if returnModels:
        return quality, predictions, models, quality_std
    else:
        return quality, predictions, quality_std


def check_done(allTrys, columns):
    import itertools
    for cs in list(itertools.permutations(columns)):
        s = '|'.join(cs)
        if s in allTrys: return True
    allTrys.append('|'.join(columns))
    return False


def descriptor_quality(data, label_names, all_features, feature_subset_size=2, cv_parts_count=10, cv_repeat=5, unknown_data=None, model=None, folder='quality_by_label', printDebug=False):
    """Calculate cross-validation result for all feature subsets of the given size for all labels
    
    Args:
        data (pandas dataframe):  data with desctriptors and labels
        label_names: list of label names to predict
        all_features (list of strings): features from which subsets are taken
        feature_subset_size (int, optional): size of subsets
        cv_parts_count (int, optional): cross validation count (divide all data into cv_parts_count parts)
        cv_repeat: repeat cross validation times
        unknown_data: features with unknown labels to make prediction
        model: ML model to use
        folder (str, optional): output folder to save results
        printDebug (boolean): show debug output
    """
    qualities = {}
    for label in label_names: qualities[label] = []
    allTrys = []
    for fs in itertools.product(*([all_features]*feature_subset_size)):
        if (len(set(fs)) != feature_subset_size) or check_done(allTrys, fs):
            continue
        # if len(qualities[label])>=2: continue
        fs = list(fs)
        res = getQuality(data, fs, label_names, model0=model, cv_count=cv_parts_count, m=cv_repeat, returnModels=unknown_data is not None, printDebug=printDebug)
        quality = res[0]
        if unknown_data is None: quality_std = res[2]
        else:
            models = res[2]
            quality_std = res[3]

        for label in quality:
            res_d = {'features':' + '.join(fs), 'quality':quality[label], 'quality_std':quality_std[label]}
            if unknown_data is not None:
                tmp_model = models[label]
                res_d['predictions'] = tmp_model.predict(unknown_data.loc[:,fs])
            qualities[label].append(res_d)
    os.makedirs(folder, exist_ok=True)
    for label in qualities:
        results = sorted(qualities[label], key=lambda res_list: res_list['quality'], reverse=True)
        with open(folder+os.sep+label+'.txt', 'w') as f:
            for r in results:
                s = f"{r['quality']:.3f}±{r['quality_std']:.3f}  -  {r['features']}\n"
                f.write(s)
                print(s[:-1])
                if unknown_data is not None:
                    pred = " ".join([f"{i}:{r['predictions'][i]:.2f}" for i in range(unknown_data.shape[0])])
                    pred = f"predictions: {pred}\n"
                    f.write(pred)
                    print(pred[:-1])


def plot_descriptors_2d(data, descriptor_names, label_names, labelMaps=None, folder_prefix='', unknown=None, markersize=500, textsize=8, alpha=1, cv_count=2, plot_data_only=False, doNotPlotRemoteCount=0):
    """Plot 2d prediction map.
    
    Args:
        data (pandas dataframe):  data with desctriptors and labels
        descriptor_names (list - pair): 2 names of descriptors to use for prediction
        label_names (list): all label names to predict
        labelMaps (dict): {label: {'valueString':number, ...}, ...} - maps of label vaules to numbers
        folder_prefix (string): output folders prefix
        plot_data_only (boolean): do not predict
        doNotPlotRemoteCount (integer): calculate mean and do not plot the most remote doNotPlotRemoteCount points
        returns: saves all graphs to two folders: folder_prefix_by_label, folder_prefix_by descriptors
    """
    assert len(descriptor_names) == 2
    if labelMaps is None: labelMaps = {}
    data = data.sample(frac=1).reset_index(drop=True)
    folder = folder_prefix + '_by_descriptors'+os.sep+descriptor_names[0]+'_'+descriptor_names[1]
    os.makedirs(folder, exist_ok=True)
    folder2 = folder_prefix+'_by_label'
    os.makedirs(folder2, exist_ok=True)
    if not plot_data_only:
        quality, predictions, models, _ = getQuality(data, descriptor_names, label_names, m=1, cv_count=cv_count, returnModels=True)

    colorMap = plotting.truncate_colormap('hsv', minval=0, maxval=0.9)
    x = data[descriptor_names[0]].to_numpy()
    y = data[descriptor_names[1]].to_numpy()
    if doNotPlotRemoteCount > 0:
        xm = np.median(x); ym = np.median(y)
        x_normed = (x-xm)/np.sqrt(np.median((x-xm)**2))
        y_normed = (y - ym) / np.sqrt(np.median((y - ym) ** 2))
        ind = np.argsort(-x_normed**2-y_normed**2)
        bad_ind = ind[:doNotPlotRemoteCount]
        good_ind = np.setdiff1d(np.arange(len(x)), bad_ind)
        x = x[good_ind]
        y = y[good_ind]
    for label in label_names:
        if label not in data.columns: continue
        labelData = data[label].to_numpy()
        if doNotPlotRemoteCount > 0:
            labelData = labelData[good_ind]
        os.makedirs(folder2+os.sep+label, exist_ok=True)
        fig, ax = plotting.createfig()

        c = labelData
        assert np.all(pd.notnull(c))
        c_min = np.min(c); c_max = np.max(c)
        transform = lambda r: (r-c_min) / (c_max - c_min) * 0.9
        if not plot_data_only:
            # contours
            x_min, x_max = np.min(x), np.max(x)
            x_min, x_max = x_min-0.2*(x_max-x_min), x_max+0.2*(x_max-x_min)
            y_min, y_max = np.min(y), np.max(y)
            y_min, y_max = y_min-0.2*(y_max-y_min), y_max+0.2*(y_max-y_min)
            xx, yy =  np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))
            preds0 = models[label].predict(np.hstack((xx.reshape(-1, 1), yy.reshape(-1, 1))))
            preds = transform(preds0.reshape(xx.shape))
        # print(label,'- classification =', ML.isClassification(data, label))
        if ML.isClassification(data, label):
            ticks = np.unique(data[label])
            # levels = transform( np.append( np.unique(data[label]), data[label].max()+1 ) )
            levels = transform( np.append( ticks-0.5, np.max(ticks)+0.5 ) )
        else:
            ticks = np.linspace(data[label].min(), data[label].max(), 10)
            delta = ticks[1]-ticks[0]
            levels = np.append(ticks-delta/2, ticks[-1]+delta/2)
            levels = transform(levels)
        if not plot_data_only:
            CF = ax.contourf(xx, yy, preds, cmap=colorMap, vmin=0, vmax=1, levels=levels, extend='both')
            # save to file
            cont_data = pd.DataFrame()
            cont_data[descriptor_names[0]] = xx.reshape(-1)
            cont_data[descriptor_names[1]] = yy.reshape(-1)
            cont_data[label] = preds0.reshape(-1)
            cont_data.to_csv(folder+'/'+label+'.csv', index=False)

        # known
        c = transform(c)
        linewidth = 1 if not plot_data_only else 0
        sc = ax.scatter(x, y, markersize, c=c, cmap=colorMap, vmin=0, vmax=1, alpha=alpha, linewidth=linewidth, edgecolor='black')
        if not plot_data_only:
            c = transform(predictions[label])
            ax.scatter(x, y, markersize/10, c=c, cmap=colorMap, vmin=0, vmax=1)
        mappable = sc if plot_data_only else CF
        cbar = fig.colorbar(mappable, ax=ax, extend='max', orientation='vertical', ticks=levels[:-1], format='%.1g')
        if label in labelMaps:
            cbarTicks = [None]*len(labelMaps[label])
            for name in labelMaps[label]:
                cbarTicks[labelMaps[label][name]] = name
            cbar.ax.set_yticklabels(cbarTicks)
        else: cbar.ax.set_yticklabels(ticks)

        # unknown
        if unknown is not None:
            if not plot_data_only:
                pred_unk = models[label].predict(unknown.loc[:,descriptor_names])
                c_params = {'c':transform(pred_unk), 'cmap':colorMap}
            else: c_params = {'c':'white'}
            ax.scatter(unknown[descriptor_names[0]], unknown[descriptor_names[1]], markersize, **c_params, vmin=0, vmax=1, edgecolor='black')
            for i in range(len(unknown)):
                ax.text(unknown.loc[i, descriptor_names[0]], unknown.loc[i, descriptor_names[1]], f'u{i}', ha='center', va='center', size=np.sqrt(markersize)/2)

        # text
        if textsize>0:
            for i in range(data.shape[0]):
                if i not in good_ind: continue
                ind = i  # data.loc[i,'Ind']
                ax.text(data.loc[i,descriptor_names[0]], data.loc[i,descriptor_names[1]], str(ind), ha='center', va='center', size=textsize)

        ax.set_xlabel(descriptor_names[0])
        ax.set_ylabel(descriptor_names[1])
        if not plot_data_only:
            qs = "{:.2f}".format(quality[label])
            ax.set_title(label+' prediction. Acc = '+qs)
            qs += '_'
        else: 
            ax.set_title(label)
            qs = ''
        ax.set_xlim(plotting.getPlotLim(x))
        ax.set_ylim(plotting.getPlotLim(y))
        plotting.savefig(folder+'/'+label+'.png', fig)
        plotting.savefig(folder2+os.sep+label+os.sep+qs+descriptor_names[0]+'_'+descriptor_names[1]+'.png', fig)
        plotting.closefig(fig)


@ignore_warnings(category=ConvergenceWarning)
def getLinearAnalyticModel(data, features, label, l1_ratio, try_alpha=None, cv_count=10):
    if try_alpha is None: try_alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]
    data = data.sample(frac=1).reset_index(drop=True)
    label_data = data[label]
    scores = np.zeros(len(try_alpha))
    models = [None] * len(try_alpha)
    for i in range(len(try_alpha)):
        models[i] = sklearn.linear_model.ElasticNet(alpha=try_alpha[i], l1_ratio=l1_ratio, fit_intercept=True, normalize=False)
        scores[i] = np.mean(sklearn.model_selection.cross_val_score(models[i], data.loc[:, features], label_data, cv=cv_count))
    i_best = np.argmax(scores)
    model = models[i_best]
    score = scores[i_best]
    if score > 0.5:
        model.fit(data.loc[:, features], label_data)
        model_str = ''
        for i in range(len(features)):
            if abs(model.coef_[i]) >= 0.005:
                model_str += f'{model.coef_[i]:+.2f}*' + features[i]
        if abs(model.intercept_) >= 0.005:
            model_str += f'{model.intercept_:+.2f}'
        model_string = f'Score={scores[i_best]:.2f}. Model: {label} = {model_str}'
    else: model_string = f'No linear formula for {label}'
    return score, model, model_string


@ignore_warnings(category=ConvergenceWarning)
def getAnalyticFormulasForGivenFeatures(data0, features0, label_names, l1_ratio=1, try_alpha=None, cv_count=10, normalize=True, output_file='formulas.txt'):
    """
    Finds analytical formulas for labels in terms of linear and quadratic functions of features. Data is normilized to zero mean and std=1.
    Algorithm. For each label we check whether the label can be expressed in terms of features using general nonlinear model (ExtraTreesRegressor). If score>0.5 we try to build linear model using feature selecting algoritm Elastic Net. If its score>0.5 we print it. The linear formula returned by Elastic Net is also heavy, so we sort coefficients by absolute value and try to build linear model based on subsets of features with the largest absolute coeffitients. The attempts are done for subsets of each size: 1, 2, 3, ...
    :param data0: data frame
    :param features0: feature names
    :param label_names:
    :param l1_ratio:
    :param try_alpha:
    :param cv_count:
    :param normalize:
    :param output_file:
    :return:
    """
    if try_alpha is None: try_alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]
    data0 = data0.sample(frac=1).reset_index(drop=True)
    if isinstance(features0, list):
        features0 = np.array(features0)
    if normalize:
        mean = data0.mean()
        std = data0.std()
        data = (data0-mean)/std
    data2 = ML.transformFeatures2Quadric(data.loc[:, features0], addConst=False)
    data2_features = np.array(data2.columns)
    for label in label_names: data2[label] = data[label]
    dataSets = [data, data2]
    featureSets = [features0, data2_features]
    result_file = open(output_file, 'w')
    for label in label_names:
        # label_data = (data[label]+data[label].min())**2
        label_data = data[label]
        for di in range(len(dataSets)):
            d = dataSets[di]
            features = featureSets[di]
            # check possibility
            model = ExtraTreesRegressor(n_estimators=100, random_state=0, min_samples_leaf=10)
            # model = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.02, n_estimators=100)
            score = np.mean(sklearn.model_selection.cross_val_score(model, d, label_data, cv=cv_count))
            if score <= 0.5:
                model_string = f'{label} can\'t be expressed in terms of features: '+str(features)
            else:
                score, model, model_string = getLinearAnalyticModel(d, features, label, l1_ratio=l1_ratio, try_alpha=try_alpha, cv_count=cv_count)
            print(model_string)
            result_file.write(model_string+'\n')

            if score > 0.5:
                # get simple models
                ind = np.argsort(np.abs(model.coef_))
                max_print_num = 5
                print_num = 0
                for i in range(len(ind)-1):
                    fs = features[ind[-i-1:]]
                    score, model, model_string = getLinearAnalyticModel(d, fs, label, l1_ratio=l1_ratio, try_alpha=try_alpha, cv_count=cv_count)
                    if score > 0.5:
                        print(' '*8 + model_string)
                        result_file.write(' '*8 + model_string + '\n')
                        print_num += 1
                        if print_num > max_print_num: break
    result_file.close()


